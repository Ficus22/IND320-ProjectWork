{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dfe367",
   "metadata": {},
   "source": [
    "# **IND320 Project Log** ‚Äì Assignment 4: Machine Learning\n",
    "\n",
    "**Esteban Carrasco**\n",
    "*November 20, 2025*\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Project Overview**\n",
    "This project focused on analyzing **hourly electricity production and consumption data** from Elhub for Norwegian price areas. Using Python (Pandas, NumPy, SciPy, Plotly), the goal was to develop interactive visualizations and predictive models to explore relationships between meteorological conditions and energy dynamics.\n",
    "\n",
    "The dataset, provided in CSV format, required preprocessing to handle temporal variations and diverse production types (hydro, wind, etc.). The analysis was deployed as a **Streamlit web application** for user-friendly exploration.\n",
    "\n",
    "**Links**\n",
    "- **Streamlit App**: [ind320-projectwork-esteban-carrasco.streamlit.app](https://ind320-projectwork-esteban-carrasco.streamlit.app)\n",
    "- **GitHub Repository**: [github.com/Ficus22/IND320-ProjectWork](https://github.com/Ficus22/IND320-ProjectWork)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Development Process**\n",
    "\n",
    "### **2.1 Data Handling and Objectives**\n",
    "The project extended the Streamlit app with new features, including interactive maps, sliding window correlation analysis, and dynamic forecasting.\n",
    "\n",
    "Data from **2021‚Äì2024** was retrieved via the Elhub API for both production (`PRODUCTION_PER_GROUP_MBA_HOUR`) and consumption (`CONSUMPTION_PER_GROUP_MBA_HOUR`). Due to storage constraints, Cassandra was not used, and MongoDB was the primary database for storing and querying the data.\n",
    "\n",
    "Key objectives included:\n",
    "- Implementing **interactive map visualizations** with GeoJSON overlays for Norwegian price areas (NO1‚ÄìNO5).\n",
    "- Developing **snow drift calculations** and visualizations per year and month.\n",
    "- Enabling **meteorology-energy correlation analysis** using sliding windows.\n",
    "- Adding **SARIMAX-based forecasting** for energy production and consumption.\n",
    "- Improving user experience with caching, spinners, and intuitive page organization.\n",
    "\n",
    "#### **Challenges and Solutions**\n",
    "The main challenge was **storage limitations**, which prevented the use of Cassandra. Instead, MongoDB was used to store and manage the dataset efficiently. Collaboration with classmates helped validate STL decomposition parameters and frequency analysis approaches.\n",
    "\n",
    "### **2.2 AI Assistance**\n",
    "*Le Chat* ([Mistral AI](https://mistral.ai)) provided support in:\n",
    "- Designing the **interactive map interface** with Plotly and Folium.\n",
    "- Integrating **snow drift calculations** into Streamlit with dynamic plotting.\n",
    "- Converting the **sliding window correlation** analysis into an interactive Streamlit module.\n",
    "- Implementing the **SARIMAX forecasting interface** and visualizations.\n",
    "- Explaining SARIMAX parameters and translating the project documentation into English.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Streamlit Phase**\n",
    "\n",
    "### **Features Added**\n",
    "The Streamlit app was enhanced with several key features:\n",
    "\n",
    "**Interactive Map & Selectors**\n",
    "A Plotly/Folium-based map with GeoJSON overlays allows users to select price areas, view coordinates, and analyze production/consumption groups over custom time intervals. The map highlights selected areas and stores user-clicked coordinates for further analysis. Choropleth coloring reflects mean energy values, improving interpretability.\n",
    "\n",
    "**Snow Drift Calculation & Visualization**\n",
    "Snow drift was calculated from **July 1 to June 30** of the following year to align with seasonal patterns. Users can select a range of years and visualize results with wind roses and monthly plots.\n",
    "\n",
    "**Meteorology & Energy Production Correlation**\n",
    "A **sliding window correlation** tool was integrated to explore how meteorological variables (e.g., temperature, wind speed) influence energy production. Users can adjust lag and window length to detect delayed effects and compare correlations during normal vs. extreme weather conditions.\n",
    "\n",
    "The following plots illustrate the relationship between **temperature and energy production** over time:\n",
    "\n",
    "1. **Lag Effect Scan**\n",
    "   ![Lag Effect Scan Plot](LagPlot.png)\n",
    "   The plot shows how correlation varies across different lags, revealing cyclical patterns that align with daily and seasonal cycles. Positive peaks indicate periods where temperature changes strongly influence energy production, while negative troughs suggest inverse relationships.\n",
    "\n",
    "2. **Aligned Time-Series (Normalized)**\n",
    "   ![Aligned Time-Series (Normalized) Plot](AllignedPlot.png)\n",
    "   This plot displays normalized temperature and energy production from **2021 to 2024**, highlighting seasonal trends and short-term fluctuations. The alignment of spikes and dips suggests a dynamic relationship, where temperature changes often precede shifts in energy production.\n",
    "\n",
    "**Discussion on Correlation Over Time**\n",
    "The sliding window analysis reveals that temperature and energy production exhibit time-dependent correlations. During warmer months, higher temperatures often correlate with increased energy production (e.g., solar output), while colder periods may show inverse trends due to reduced efficiency or demand shifts. The lag scan further confirms that temperature impacts energy production with a delay of several hours, likely due to system response times. These insights underscore the importance of time-lagged modeling in energy forecasting.\n",
    "\n",
    "**Forecasting Energy Production & Consumption**\n",
    "A dynamic SARIMAX interface allows users to define training periods, forecast horizons, and exogenous variables (e.g., weather data). The model provides interactive forecasts with confidence intervals, making it accessible for both experts and non-experts.\n",
    "\n",
    "**UX Improvements**\n",
    "Pages were reordered for better workflow, and caching was implemented to speed up computations. Spinners were added to indicate progress during long calculations, enhancing the overall user experience.\n",
    "\n",
    "### **Challenges and Adaptations**\n",
    "- Ensured smooth **map interactions** with coordinate selections and area highlighting.\n",
    "- Adjusted **snow drift visualizations** for dynamic year and month selection.\n",
    "- Maintained **consistent color scales** across choropleth maps for clarity.\n",
    "- Validated **meteorological and production dataset compatibility** for accurate correlation analysis.\n",
    "- Simplified the **forecasting interface** with input validation to accommodate non-expert users.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Jupyter Notebook Phase**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa6f44",
   "metadata": {},
   "source": [
    "For html rndering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f14ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483061cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.1.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01351098",
   "metadata": {},
   "source": [
    "## Elhub API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf730d",
   "metadata": {},
   "source": [
    "Testing API connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056dfb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "entity =\"price-areas\"\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "URL = f\"https://api.elhub.no/energy-data/v0/{entity}?dataset={dataset}\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116dcdf5",
   "metadata": {},
   "source": [
    "Fetching data from Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c8c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total production records fetched (2022-2024): 657600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "ENTITY = \"price-areas\"\n",
    "DATASET_PRODUCTION = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_PRODUCTION = [2022, 2023, 2024]\n",
    "\n",
    "def generate_monthly_ranges(year):\n",
    "    start_year = datetime(year, 1, 1)\n",
    "    end_year = datetime(year, 12, 31)\n",
    "    ranges = []\n",
    "    current = start_year\n",
    "    while current <= end_year:\n",
    "        month_start = current\n",
    "        month_end = (current + relativedelta(months=1)) - relativedelta(seconds=1)\n",
    "        start_str = month_start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        end_str = month_end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        ranges.append((start_str, end_str))\n",
    "        current += relativedelta(months=1)\n",
    "    return ranges\n",
    "\n",
    "all_production_records = []\n",
    "\n",
    "for year in YEARS_PRODUCTION:\n",
    "    monthly_ranges = generate_monthly_ranges(year)\n",
    "    for start_date, end_date in monthly_ranges:\n",
    "        url = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={DATASET_PRODUCTION}&startDate={start_date}&endDate={end_date}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for entry in data.get(\"data\", []):\n",
    "                records = entry.get(\"attributes\", {}).get(\"productionPerGroupMbaHour\", [])\n",
    "                all_production_records.extend(records)\n",
    "            #print(f\"Fetched {len(records)} records for {start_date[:10]} ({year}).\")\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {start_date[:10]} ({year}).\")\n",
    "\n",
    "print(f\"Total production records fetched (2022-2024): {len(all_production_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff62d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1:\n",
      "{'endTime': '2022-01-01T01:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1291422.4, 'startTime': '2022-01-01T00:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 2:\n",
      "{'endTime': '2022-01-01T02:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1246209.4, 'startTime': '2022-01-01T01:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 3:\n",
      "{'endTime': '2022-01-01T03:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1271757.0, 'startTime': '2022-01-01T02:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 4:\n",
      "{'endTime': '2022-01-01T04:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1204251.8, 'startTime': '2022-01-01T03:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 5:\n",
      "{'endTime': '2022-01-01T05:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1202086.9, 'startTime': '2022-01-01T04:00:00+01:00'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display the head\n",
    "for i, record in enumerate(all_production_records[:5]):\n",
    "    print(f\"Record {i+1}:\")\n",
    "    print(record)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7fda6",
   "metadata": {},
   "source": [
    "## Adding data into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903864bf",
   "metadata": {},
   "source": [
    "Connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208f7224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the URI from environment variables\n",
    "uri = os.getenv(\"MONGO_URI\")\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052c8d1",
   "metadata": {},
   "source": [
    "Transform API records to match existing collection format using a dataframe to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67388d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_area                  object\n",
      "production_group            object\n",
      "start_time          datetime64[ns]\n",
      "quantity_kwh               float64\n",
      "dtype: object\n",
      "  price_area production_group          start_time  quantity_kwh\n",
      "0        NO1            hydro 2022-01-01 00:00:00     1291422.4\n",
      "1        NO1            hydro 2022-01-01 01:00:00     1246209.4\n",
      "2        NO1            hydro 2022-01-01 02:00:00     1271757.0\n",
      "3        NO1            hydro 2022-01-01 03:00:00     1204251.8\n",
      "4        NO1            hydro 2022-01-01 04:00:00     1202086.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cr√©er un DataFrame directement depuis la liste de dicts\n",
    "df = pd.DataFrame(all_production_records)\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "# Rename columns and keep only the necessary ones\n",
    "df = df.rename(columns={\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"productionGroup\": \"production_group\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"quantityKwh\": \"quantity_kwh\"\n",
    "})[[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]]\n",
    "\n",
    "df[\"start_time\"] = pd.to_datetime(df[\"start_time\"].str[:-6])\n",
    "\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84568e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657600 documents inserted into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MongoDB\n",
    "load_dotenv()\n",
    "client = MongoClient(os.getenv(\"MONGO_URI\"), server_api=ServerApi('1'))\n",
    "db = client[\"elhub_data\"]\n",
    "collection = db[\"production_data\"]\n",
    "\n",
    "# Convert the DataFrame to a dictionary and insert\n",
    "data_for_mongo = df.to_dict(\"records\")\n",
    "collection.insert_many(data_for_mongo)\n",
    "print(f\"{len(data_for_mongo)} documents inserted into MongoDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b57f28",
   "metadata": {},
   "source": [
    "## Adding data into Cassandra table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eada5d",
   "metadata": {},
   "source": [
    "Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b3c080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Cassandra.\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "import sys\n",
    "\n",
    "KEYSPACE = \"my_ind320_keyspace\"\n",
    "TABLE = \"elhub_data\"\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "CONCURRENCY_LEVEL = 100  # number of simultaneous inserts\n",
    "\n",
    "try:\n",
    "    cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "    session = cluster.connect()\n",
    "    print(\"‚úÖ Connected to Cassandra.\")\n",
    "except Exception as e:\n",
    "    sys.exit(f\"‚ùå Cassandra connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6c1a5",
   "metadata": {},
   "source": [
    "Connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec62284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elhub_data\n",
      "my_first_table\n"
     ]
    }
   ],
   "source": [
    "session.set_keyspace('my_ind320_keyspace')\n",
    "rows = session.execute(\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'my_ind320_keyspace';\")\n",
    "for row in rows:\n",
    "    print(row.table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8639705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_area                  object\n",
      "production_group            object\n",
      "start_time          datetime64[ns]\n",
      "quantity_kwh               float64\n",
      "dtype: object\n",
      "  price_area production_group          start_time  quantity_kwh\n",
      "0        NO1            hydro 2022-01-01 00:00:00     1291422.4\n",
      "1        NO1            hydro 2022-01-01 01:00:00     1246209.4\n",
      "2        NO1            hydro 2022-01-01 02:00:00     1271757.0\n",
      "3        NO1            hydro 2022-01-01 03:00:00     1204251.8\n",
      "4        NO1            hydro 2022-01-01 04:00:00     1202086.9\n",
      "657600\n"
     ]
    }
   ],
   "source": [
    "df_cassandra = df\n",
    "\n",
    "print(df_cassandra.dtypes)\n",
    "print(df_cassandra.head())\n",
    "print(len(df_cassandra))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1dc2b",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Inserting data into Cassandra...\n"
     ]
    },
    {
     "ename": "WriteTimeout",
     "evalue": "Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message=\"Operation timed out - received only 0 responses.\" info={'consistency': 'LOCAL_ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'SIMPLE'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWriteTimeout\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     13\u001b[39m params = [\n\u001b[32m     14\u001b[39m     (\n\u001b[32m     15\u001b[39m         row[\u001b[33m\"\u001b[39m\u001b[33mprice_area\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows()\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Use tqdm for progress tracking\u001b[39;00m\n\u001b[32m     24\u001b[39m results = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m     25\u001b[39m     tqdm(\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[43mexecute_concurrent_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONCURRENCY_LEVEL\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     29\u001b[39m         total=\u001b[38;5;28mlen\u001b[39m(params),\n\u001b[32m     30\u001b[39m         desc=\u001b[33m\"\u001b[39m\u001b[33mInsertion\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m     )\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Check for potential errors\u001b[39;00m\n\u001b[32m     35\u001b[39m errors = [res \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res[\u001b[32m0\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\concurrent.py:167\u001b[39m, in \u001b[36mexecute_concurrent_with_args\u001b[39m\u001b[34m(session, statement, parameters, *args, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_concurrent_with_args\u001b[39m(session, statement, parameters, *args, **kwargs):\n\u001b[32m    164\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m    See :meth:`.Session.execute_concurrent_with_args`.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecute_concurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\concurrent.py:42\u001b[39m, in \u001b[36mexecute_concurrent\u001b[39m\u001b[34m(session, statements_and_parameters, concurrency, raise_on_first_error, results_generator, execution_profile)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     40\u001b[39m executor = ConcurrentExecutorGenResults(session, statements_and_parameters, execution_profile) \\\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results_generator \u001b[38;5;28;01melse\u001b[39;00m ConcurrentExecutorListResults(session, statements_and_parameters, execution_profile)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_first_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\concurrent.py:138\u001b[39m, in \u001b[36mConcurrentExecutorListResults.execute\u001b[39m\u001b[34m(self, concurrency, fail_fast)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, concurrency, fail_fast):\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mConcurrentExecutorListResults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfail_fast\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\concurrent.py:69\u001b[39m, in \u001b[36m_ConcurrentExecutor.execute\u001b[39m\u001b[34m(self, concurrency, fail_fast)\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_next():\n\u001b[32m     68\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\concurrent.py:156\u001b[39m, in \u001b[36mConcurrentExecutorListResults._results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m._condition.wait()\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fail_fast:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fail_fast:  \u001b[38;5;66;03m# raise the exception even if there was no wait\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n",
      "\u001b[31mWriteTimeout\u001b[39m: Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message=\"Operation timed out - received only 0 responses.\" info={'consistency': 'LOCAL_ONE', 'required_responses': 1, 'received_responses': 0, 'write_type': 'SIMPLE'}"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "insert_query = session.prepare(f\"\"\"\n",
    "    INSERT INTO {TABLE} (price_area, production_group, start_time, quantity_kwh)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"üöÄ Inserting data into Cassandra...\")\n",
    "\n",
    "params = [\n",
    "    (\n",
    "        row[\"price_area\"],\n",
    "        row[\"production_group\"],\n",
    "        row[\"start_time\"].to_pydatetime(),\n",
    "        float(row[\"quantity_kwh\"])\n",
    "    )\n",
    "    for _, row in df_cassandra.iterrows()\n",
    "]\n",
    "\n",
    "# Use tqdm for progress tracking\n",
    "results = list(\n",
    "    tqdm(\n",
    "        execute_concurrent_with_args(\n",
    "            session, insert_query, params, concurrency=CONCURRENCY_LEVEL\n",
    "        ),\n",
    "        total=len(params),\n",
    "        desc=\"Insertion\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check for potential errors\n",
    "errors = [res for res in results if not res[0]]\n",
    "if errors:\n",
    "    print(f\"‚ö†Ô∏è {len(errors)} insertion errors detected.\")\n",
    "else:\n",
    "    print(\"‚úÖ All data inserted successfully!\")\n",
    "\n",
    "\n",
    "cluster.shutdown()\n",
    "print(\"\\nüèÅ Import completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f98280",
   "metadata": {},
   "source": [
    "## Adding a new table into databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affbc0c",
   "metadata": {},
   "source": [
    "Fetching data from Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31d7e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total consumption records fetched (2021‚Äì2024): 876600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# CONSTANTS FOR CONSUMPTION\n",
    "ENTITY = \"price-areas\"\n",
    "DATASET_CONSUMPTION = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_CONSUMPTION = [2021, 2022, 2023, 2024]\n",
    "\n",
    "\n",
    "# FUNCTION TO GENERATE MONTHLY DATE RANGES\n",
    "def generate_monthly_ranges(year):\n",
    "    start_year = datetime(year, 1, 1)\n",
    "    end_year = datetime(year, 12, 31)\n",
    "    ranges = []\n",
    "    current = start_year\n",
    "    while current <= end_year:\n",
    "        month_start = current\n",
    "        month_end = (current + relativedelta(months=1)) - relativedelta(seconds=1)\n",
    "        # Formatting for URL encoding +01:00\n",
    "        start_str = month_start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        end_str = month_end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        ranges.append((start_str, end_str))\n",
    "        current += relativedelta(months=1)\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# FETCH HOURLY CONSUMPTION DATA 2021 - 2024\n",
    "all_consumption_records = []\n",
    "\n",
    "for year in YEARS_CONSUMPTION:\n",
    "    monthly_ranges = generate_monthly_ranges(year)\n",
    "    for start_date, end_date in monthly_ranges:\n",
    "        url = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={DATASET_CONSUMPTION}&startDate={start_date}&endDate={end_date}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for entry in data.get(\"data\", []):\n",
    "                records = entry.get(\"attributes\", {}).get(\"consumptionPerGroupMbaHour\", [])\n",
    "                all_consumption_records.extend(records)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {start_date[:10]} ({year}).\")\n",
    "\n",
    "print(f\"Total consumption records fetched (2021‚Äì2024): {len(all_consumption_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140c9bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T01:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 177071.56, 'startTime': '2021-01-01T00:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 2:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T02:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 171335.12, 'startTime': '2021-01-01T01:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 3:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T03:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 164912.02, 'startTime': '2021-01-01T02:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 4:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T04:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 160265.77, 'startTime': '2021-01-01T03:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 5:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T05:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 159828.69, 'startTime': '2021-01-01T04:00:00+01:00'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display the head\n",
    "for i, record in enumerate(all_consumption_records[:5]):\n",
    "    print(f\"Record {i+1}:\")\n",
    "    print(record)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bf381",
   "metadata": {},
   "source": [
    "#### In MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esteban\\AppData\\Local\\Temp\\ipykernel_10180\\2678956536.py:21: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_cons[\"start_time\"] = pd.to_datetime(df_cons[\"start_time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_area               object\n",
      "consumption_group        object\n",
      "start_time               object\n",
      "quantity_kwh            float64\n",
      "metering_point_count      int64\n",
      "dtype: object\n",
      "  price_area consumption_group                 start_time  quantity_kwh  \\\n",
      "0        NO1             cabin  2021-01-01 00:00:00+01:00     177071.56   \n",
      "1        NO1             cabin  2021-01-01 01:00:00+01:00     171335.12   \n",
      "2        NO1             cabin  2021-01-01 02:00:00+01:00     164912.02   \n",
      "3        NO1             cabin  2021-01-01 03:00:00+01:00     160265.77   \n",
      "4        NO1             cabin  2021-01-01 04:00:00+01:00     159828.69   \n",
      "\n",
      "   metering_point_count  \n",
      "0                100607  \n",
      "1                100607  \n",
      "2                100607  \n",
      "3                100607  \n",
      "4                100607  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cons = pd.DataFrame(all_consumption_records)\n",
    "\n",
    "# Keep only needed fields and rename to target format\n",
    "df_cons = df_cons.rename(columns={\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"consumptionGroup\": \"consumption_group\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"quantityKwh\": \"quantity_kwh\",\n",
    "    \"meteringPointCount\": \"metering_point_count\"\n",
    "})[[\n",
    "    \"price_area\",\n",
    "    \"consumption_group\",\n",
    "    \"start_time\",\n",
    "    \"quantity_kwh\",\n",
    "    \"metering_point_count\"\n",
    "]]\n",
    "\n",
    "# Convert start_time to datetime\n",
    "df_cons[\"start_time\"] = pd.to_datetime(df_cons[\"start_time\"], utc=True)\n",
    "\n",
    "print(df_cons.dtypes)\n",
    "print(df_cons.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8a4a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876600 hourly consumption records inserted (2021‚Äì2024).\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = MongoClient(os.getenv(\"MONGO_URI\"), server_api=ServerApi('1'))\n",
    "db = client[\"elhub_data\"]\n",
    "collection = db[\"consumption_data\"]  # NEW TABLE (COLLECTION)\n",
    "\n",
    "# Transform DataFrame to list of dicts and insert\n",
    "data_for_mongo = df_cons.to_dict(\"records\")\n",
    "collection.insert_many(data_for_mongo)\n",
    "\n",
    "print(f\"{len(data_for_mongo)} hourly consumption records inserted (2021‚Äì2024).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dc70f",
   "metadata": {},
   "source": [
    "#### In Cassandra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
