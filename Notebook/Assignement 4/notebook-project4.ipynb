{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6dfe367",
   "metadata": {},
   "source": [
    "# **IND320 Project Log** - Assignement 3 : Electricity Production Data Analysis\n",
    "**Esteban Carrasco**  \n",
    "*November 07, 2025*\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Project Overview**\n",
    "\n",
    "This project aimed to analyze **hourly electricity production data** from Elhub for Norwegian price areas using Python (Pandas, NumPy, SciPy, Plotly) and deploy interactive visualizations. The dataset, provided in CSV format, required preprocessing for temporal analysis and multi-scale visualization due to varying production types (hydro, wind, etc.).\n",
    "\n",
    "#### Links\n",
    "\n",
    "* **Streamlit App** : [see here](https://ind320-projectwork-esteban-carrasco.streamlit.app)\n",
    "* **Github** : [see here](https://github.com/Ficus22/IND320-ProjectWork)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Development Process**\n",
    "\n",
    "### **2.1 Log:**\n",
    "\n",
    "**Objective**:\n",
    "Analyze hourly electricity production in Norway (price areas NO1‚ÄìNO5) for 2019‚Äì2021, focusing on production groups such as hydro and wind. The goal was to create reproducible visualizations, detect anomalies, and generate interactive plots including seasonal decomposition and spectrograms.\n",
    "\n",
    "---\n",
    "\n",
    "#### **A- Data Preparation**\n",
    "\n",
    "1. **Price Areas and Locations**:\n",
    "   Created a Pandas DataFrame containing the five electricity price areas: **Oslo, Kristiansand, Trondheim, Troms√∏, Bergen**, along with their longitude and latitude coordinates (geographical center points).\n",
    "\n",
    "2. **Data Loading and Cleaning**:\n",
    "\n",
    "   * Loaded CSV production data using Pandas.\n",
    "   * Ensured `start_time` was converted to datetime for indexing.\n",
    "   * Checked for missing or duplicate values.\n",
    "   * Filtered by price area and production group for analysis.\n",
    "\n",
    "**Challenge**:\n",
    "Column names sometimes contained extra spaces; added stripping of whitespace to prevent key errors (`KeyError: 'price_area'`).\n",
    "\n",
    "---\n",
    "\n",
    "#### **B- Outlier Detection and Anomaly Analysis**\n",
    "\n",
    "1. **Temperature Analysis for Bergen** (historical weather data via open-meteo API):\n",
    "\n",
    "   * Downloaded ERA5 reanalysis data for 2019 using a custom function that takes longitude, latitude, and year as input.\n",
    "   * Plotted raw hourly temperature.\n",
    "   * Applied Direct Cosine Transform (DCT) high-pass filtering to create seasonally adjusted temperature variations (SATV).\n",
    "   * Used robust statistics to define control limits and highlight outliers.\n",
    "\n",
    "2. **Precipitation Analysis**:\n",
    "\n",
    "   * Plotted hourly precipitation.\n",
    "   * Detected anomalies using the Local Outlier Factor (LOF) method.\n",
    "   * Parameterized the proportion of outliers (default 1%).\n",
    "\n",
    "**Outcome**:\n",
    "Both functions returned plots and summaries of detected anomalies. Outliers were visually highlighted in contrasting colors.\n",
    "\n",
    "---\n",
    "\n",
    "#### **C- Seasonal Decomposition (STL)**\n",
    "\n",
    "* Performed STL decomposition on Elhub production data.\n",
    "* Parameters included price area, production group, seasonal period, seasonal smoother, trend smoother, and robust option.\n",
    "* Plotted observed, trend, seasonal, and residual components.\n",
    "* Wrapped in a function returning the decomposition figure for testing and reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "#### **D- Spectrogram Analysis**\n",
    "\n",
    "* Implemented a spectrogram function using SciPy and Plotly to visualize periodic patterns in electricity production.\n",
    "* Parameters: price area, production group, window length, window overlap, and colorscale.\n",
    "* Handled missing timestamps with interpolation.\n",
    "* Returned an interactive Plotly heatmap of power spectral density over time.\n",
    "\n",
    "**Example Output**:\n",
    "\n",
    "* Bergen hydro production spectrogram revealed daily and weekly cycles, with stronger intensity during winter months.\n",
    "\n",
    "---\n",
    "\n",
    "#### **E- Function Wrapping and Testing**\n",
    "\n",
    "* Each analysis step (outliers, STL decomposition, spectrogram) was encapsulated in reusable Python functions.\n",
    "* Functions were tested with default parameters and plotted for selected areas (Bergen, NO1).\n",
    "\n",
    "---\n",
    "\n",
    "#### **F- Challenges and Solutions**\n",
    "\n",
    "| Issue                                      | Solution                                                              |\n",
    "| ------------------------------------------ | --------------------------------------------------------------------- |\n",
    "| KeyError on column access                  | Stripped whitespace from CSV headers                                  |\n",
    "| Missing timestamps in hourly data          | Interpolated missing hours with `time` method                         |\n",
    "| Large spectrogram arrays slowing rendering | Optimized with smaller window overlaps and `zsmooth=\"best\"` in Plotly |\n",
    "| Seasonal trend affecting outlier detection | Used high-pass DCT to isolate short-term anomalies                    |\n",
    "\n",
    "**Collaboration**:\n",
    "Shared code snippets with classmates to validate STL parameters and frequency analysis approach.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 AI Assistance**:\n",
    "\n",
    "*Le Chat* ([Mistral AI](https://mistral.ai/)) helped optimize Pandas data filtering and datetime handling, suggested approaches for spectrogram computation, and provided guidance on anomaly detection using DCT and LOF. It also helped translate project documentation into English.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Jupyter Notebook Phase**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f14ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483061cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.1.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01351098",
   "metadata": {},
   "source": [
    "## Elhub API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaf730d",
   "metadata": {},
   "source": [
    "Testing API connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056dfb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "entity =\"price-areas\"\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "URL = f\"https://api.elhub.no/energy-data/v0/{entity}?dataset={dataset}\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116dcdf5",
   "metadata": {},
   "source": [
    "Fetching data from Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c8c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total production records fetched (2022-2024): 657600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "ENTITY = \"price-areas\"\n",
    "DATASET_PRODUCTION = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_PRODUCTION = [2022, 2023, 2024]\n",
    "\n",
    "def generate_monthly_ranges(year):\n",
    "    start_year = datetime(year, 1, 1)\n",
    "    end_year = datetime(year, 12, 31)\n",
    "    ranges = []\n",
    "    current = start_year\n",
    "    while current <= end_year:\n",
    "        month_start = current\n",
    "        month_end = (current + relativedelta(months=1)) - relativedelta(seconds=1)\n",
    "        start_str = month_start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        end_str = month_end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        ranges.append((start_str, end_str))\n",
    "        current += relativedelta(months=1)\n",
    "    return ranges\n",
    "\n",
    "all_production_records = []\n",
    "\n",
    "for year in YEARS_PRODUCTION:\n",
    "    monthly_ranges = generate_monthly_ranges(year)\n",
    "    for start_date, end_date in monthly_ranges:\n",
    "        url = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={DATASET_PRODUCTION}&startDate={start_date}&endDate={end_date}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for entry in data.get(\"data\", []):\n",
    "                records = entry.get(\"attributes\", {}).get(\"productionPerGroupMbaHour\", [])\n",
    "                all_production_records.extend(records)\n",
    "            #print(f\"Fetched {len(records)} records for {start_date[:10]} ({year}).\")\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {start_date[:10]} ({year}).\")\n",
    "\n",
    "print(f\"Total production records fetched (2022-2024): {len(all_production_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff62d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1:\n",
      "{'endTime': '2022-01-01T01:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1291422.4, 'startTime': '2022-01-01T00:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 2:\n",
      "{'endTime': '2022-01-01T02:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1246209.4, 'startTime': '2022-01-01T01:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 3:\n",
      "{'endTime': '2022-01-01T03:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1271757.0, 'startTime': '2022-01-01T02:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 4:\n",
      "{'endTime': '2022-01-01T04:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1204251.8, 'startTime': '2022-01-01T03:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 5:\n",
      "{'endTime': '2022-01-01T05:00:00+01:00', 'lastUpdatedTime': '2025-02-01T18:02:57+01:00', 'priceArea': 'NO1', 'productionGroup': 'hydro', 'quantityKwh': 1202086.9, 'startTime': '2022-01-01T04:00:00+01:00'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display the head\n",
    "for i, record in enumerate(all_production_records[:5]):\n",
    "    print(f\"Record {i+1}:\")\n",
    "    print(record)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7fda6",
   "metadata": {},
   "source": [
    "## Adding data into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903864bf",
   "metadata": {},
   "source": [
    "Connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "208f7224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the URI from environment variables\n",
    "uri = os.getenv(\"MONGO_URI\")\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052c8d1",
   "metadata": {},
   "source": [
    "Transform API records to match existing collection format using a dataframe to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67388d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_area           object\n",
      "production_group     object\n",
      "start_time           object\n",
      "quantity_kwh        float64\n",
      "dtype: object\n",
      "  price_area production_group                 start_time  quantity_kwh\n",
      "0        NO1            hydro  2022-01-01T00:00:00+01:00     1291422.4\n",
      "1        NO1            hydro  2022-01-01T01:00:00+01:00     1246209.4\n",
      "2        NO1            hydro  2022-01-01T02:00:00+01:00     1271757.0\n",
      "3        NO1            hydro  2022-01-01T03:00:00+01:00     1204251.8\n",
      "4        NO1            hydro  2022-01-01T04:00:00+01:00     1202086.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cr√©er un DataFrame directement depuis la liste de dicts\n",
    "df = pd.DataFrame(all_production_records)\n",
    "\n",
    "#print(df.head())\n",
    "\n",
    "# Rename columns and keep only the necessary ones\n",
    "df = df.rename(columns={\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"productionGroup\": \"production_group\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"quantityKwh\": \"quantity_kwh\"\n",
    "})[[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]]\n",
    "\n",
    "\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84568e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657600 documents inserted into MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to MongoDB\n",
    "load_dotenv()\n",
    "client = MongoClient(os.getenv(\"MONGO_URI\"), server_api=ServerApi('1'))\n",
    "db = client[\"elhub_data\"]\n",
    "collection = db[\"production_data\"]\n",
    "\n",
    "# Convert the DataFrame to a dictionary and insert\n",
    "data_for_mongo = df.to_dict(\"records\")\n",
    "collection.insert_many(data_for_mongo)\n",
    "print(f\"{len(data_for_mongo)} documents inserted into MongoDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b57f28",
   "metadata": {},
   "source": [
    "## Adding data into Cassandra table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eada5d",
   "metadata": {},
   "source": [
    "Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07b3c080",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "‚ùå Cassandra connection error: ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None'), '::1:9042': ConnectionShutdown('Connection to ::1:9042 was closed')})",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m ‚ùå Cassandra connection error: ('Unable to connect to any servers', {'127.0.0.1:9042': OperationTimedOut('errors=Timed out creating connection (5 seconds), last_host=None'), '::1:9042': ConnectionShutdown('Connection to ::1:9042 was closed')})\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "import sys\n",
    "\n",
    "KEYSPACE = \"my_ind320_keyspace\"\n",
    "TABLE = \"elhub_data\"\n",
    "CASSANDRA_HOST = \"localhost\"\n",
    "CASSANDRA_PORT = 9042\n",
    "CONCURRENCY_LEVEL = 100  # number of simultaneous inserts\n",
    "\n",
    "try:\n",
    "    cluster = Cluster([CASSANDRA_HOST], port=CASSANDRA_PORT)\n",
    "    session = cluster.connect()\n",
    "    print(\"‚úÖ Connected to Cassandra.\")\n",
    "except Exception as e:\n",
    "    sys.exit(f\"‚ùå Cassandra connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6c1a5",
   "metadata": {},
   "source": [
    "Connection test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec62284",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoHostAvailable",
     "evalue": "('Unable to complete the operation against any hosts', {})",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoHostAvailable\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCREATE KEYSPACE IF NOT EXISTS my_ind320_keyspace WITH REPLICATION = \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m : \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSimpleStrategy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreplication_factor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m : 1 };\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m session.set_keyspace(\u001b[33m'\u001b[39m\u001b[33mmy_ind320_keyspace\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m rows = session.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT table_name FROM system_schema.tables WHERE keyspace_name = \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmy_ind320_keyspace\u001b[39m\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\cluster.py:2677\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, query, parameters, timeout, trace, custom_payload, execution_profile, paging_state, host, execute_as)\u001b[39m\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=_NOT_SET, trace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2635\u001b[39m             custom_payload=\u001b[38;5;28;01mNone\u001b[39;00m, execution_profile=EXEC_PROFILE_DEFAULT,\n\u001b[32m   2636\u001b[39m             paging_state=\u001b[38;5;28;01mNone\u001b[39;00m, host=\u001b[38;5;28;01mNone\u001b[39;00m, execute_as=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m    Execute the given query and synchronously wait for the response.\u001b[39;00m\n\u001b[32m   2639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2674\u001b[39m \u001b[33;03m    on a DSE cluster.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaging_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\cluster.py:5048\u001b[39m, in \u001b[36mResponseFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5046\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._final_result)\n\u001b[32m   5047\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_exception\n",
      "\u001b[31mNoHostAvailable\u001b[39m: ('Unable to complete the operation against any hosts', {})"
     ]
    }
   ],
   "source": [
    "session.set_keyspace('my_ind320_keyspace')\n",
    "rows = session.execute(\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = 'my_ind320_keyspace';\")\n",
    "for row in rows:\n",
    "    print(row.table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1dc2b",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4020585c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoHostAvailable",
     "evalue": "('Unable to complete the operation against any hosts', {})",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoHostAvailable\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_keyspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKEYSPACE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m insert_query = session.prepare(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    INSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (price_area, production_group, start_time, quantity_kwh)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m    VALUES (?, ?, ?, ?)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Inserting data into Cassandra...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\cluster.py:3432\u001b[39m, in \u001b[36mSession.set_keyspace\u001b[39m\u001b[34m(self, keyspace)\u001b[39m\n\u001b[32m   3427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_keyspace\u001b[39m(\u001b[38;5;28mself\u001b[39m, keyspace):\n\u001b[32m   3428\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3429\u001b[39m \u001b[33;03m    Set the default keyspace for all queries made through this Session.\u001b[39;00m\n\u001b[32m   3430\u001b[39m \u001b[33;03m    This operation blocks until complete.\u001b[39;00m\n\u001b[32m   3431\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3432\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUSE \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotect_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyspace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\cluster.py:2677\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, query, parameters, timeout, trace, custom_payload, execution_profile, paging_state, host, execute_as)\u001b[39m\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=_NOT_SET, trace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2635\u001b[39m             custom_payload=\u001b[38;5;28;01mNone\u001b[39;00m, execution_profile=EXEC_PROFILE_DEFAULT,\n\u001b[32m   2636\u001b[39m             paging_state=\u001b[38;5;28;01mNone\u001b[39;00m, host=\u001b[38;5;28;01mNone\u001b[39;00m, execute_as=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m    Execute the given query and synchronously wait for the response.\u001b[39;00m\n\u001b[32m   2639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2674\u001b[39m \u001b[33;03m    on a DSE cluster.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaging_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Esteban\\miniforge3\\envs\\D2D\\Lib\\site-packages\\cassandra\\cluster.py:5048\u001b[39m, in \u001b[36mResponseFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   5046\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._final_result)\n\u001b[32m   5047\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_exception\n",
      "\u001b[31mNoHostAvailable\u001b[39m: ('Unable to complete the operation against any hosts', {})"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "insert_query = session.prepare(f\"\"\"\n",
    "    INSERT INTO {TABLE} (price_area, production_group, start_time, quantity_kwh)\n",
    "    VALUES (?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"üöÄ Inserting data into Cassandra...\")\n",
    "\n",
    "params = [\n",
    "    (\n",
    "        row[\"price_area\"],\n",
    "        row[\"production_group\"],\n",
    "        row[\"start_time\"].to_pydatetime(),\n",
    "        float(row[\"quantity_kwh\"])\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Use tqdm for progress tracking\n",
    "results = list(\n",
    "    tqdm(\n",
    "        execute_concurrent_with_args(\n",
    "            session, insert_query, params, concurrency=CONCURRENCY_LEVEL\n",
    "        ),\n",
    "        total=len(params),\n",
    "        desc=\"Insertion\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Check for potential errors\n",
    "errors = [res for res in results if not res[0]]\n",
    "if errors:\n",
    "    print(f\"‚ö†Ô∏è {len(errors)} insertion errors detected.\")\n",
    "else:\n",
    "    print(\"‚úÖ All data inserted successfully!\")\n",
    "\n",
    "\n",
    "cluster.shutdown()\n",
    "print(\"\\nüèÅ Import completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f98280",
   "metadata": {},
   "source": [
    "## Adding a new table into databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9affbc0c",
   "metadata": {},
   "source": [
    "Fetching data from Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31d7e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total consumption records fetched (2021‚Äì2024): 876600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# CONSTANTS FOR CONSUMPTION\n",
    "ENTITY = \"price-areas\"\n",
    "DATASET_CONSUMPTION = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_CONSUMPTION = [2021, 2022, 2023, 2024]\n",
    "\n",
    "\n",
    "# FUNCTION TO GENERATE MONTHLY DATE RANGES\n",
    "def generate_monthly_ranges(year):\n",
    "    start_year = datetime(year, 1, 1)\n",
    "    end_year = datetime(year, 12, 31)\n",
    "    ranges = []\n",
    "    current = start_year\n",
    "    while current <= end_year:\n",
    "        month_start = current\n",
    "        month_end = (current + relativedelta(months=1)) - relativedelta(seconds=1)\n",
    "        # Formatting for URL encoding +01:00\n",
    "        start_str = month_start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        end_str = month_end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01:00\"\n",
    "        ranges.append((start_str, end_str))\n",
    "        current += relativedelta(months=1)\n",
    "    return ranges\n",
    "\n",
    "\n",
    "# FETCH HOURLY CONSUMPTION DATA 2021 - 2024\n",
    "all_consumption_records = []\n",
    "\n",
    "for year in YEARS_CONSUMPTION:\n",
    "    monthly_ranges = generate_monthly_ranges(year)\n",
    "    for start_date, end_date in monthly_ranges:\n",
    "        url = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={DATASET_CONSUMPTION}&startDate={start_date}&endDate={end_date}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for entry in data.get(\"data\", []):\n",
    "                records = entry.get(\"attributes\", {}).get(\"consumptionPerGroupMbaHour\", [])\n",
    "                all_consumption_records.extend(records)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for {start_date[:10]} ({year}).\")\n",
    "\n",
    "print(f\"Total consumption records fetched (2021‚Äì2024): {len(all_consumption_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "140c9bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T01:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 177071.56, 'startTime': '2021-01-01T00:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 2:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T02:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 171335.12, 'startTime': '2021-01-01T01:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 3:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T03:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 164912.02, 'startTime': '2021-01-01T02:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 4:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T04:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 160265.77, 'startTime': '2021-01-01T03:00:00+01:00'}\n",
      "--------------------------------------------------\n",
      "Record 5:\n",
      "{'consumptionGroup': 'cabin', 'endTime': '2021-01-01T05:00:00+01:00', 'lastUpdatedTime': '2024-12-20T10:35:40+01:00', 'meteringPointCount': 100607, 'priceArea': 'NO1', 'quantityKwh': 159828.69, 'startTime': '2021-01-01T04:00:00+01:00'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display the head\n",
    "for i, record in enumerate(all_consumption_records[:5]):\n",
    "    print(f\"Record {i+1}:\")\n",
    "    print(record)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bf381",
   "metadata": {},
   "source": [
    "#### In MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esteban\\AppData\\Local\\Temp\\ipykernel_10180\\2678956536.py:21: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df_cons[\"start_time\"] = pd.to_datetime(df_cons[\"start_time\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price_area               object\n",
      "consumption_group        object\n",
      "start_time               object\n",
      "quantity_kwh            float64\n",
      "metering_point_count      int64\n",
      "dtype: object\n",
      "  price_area consumption_group                 start_time  quantity_kwh  \\\n",
      "0        NO1             cabin  2021-01-01 00:00:00+01:00     177071.56   \n",
      "1        NO1             cabin  2021-01-01 01:00:00+01:00     171335.12   \n",
      "2        NO1             cabin  2021-01-01 02:00:00+01:00     164912.02   \n",
      "3        NO1             cabin  2021-01-01 03:00:00+01:00     160265.77   \n",
      "4        NO1             cabin  2021-01-01 04:00:00+01:00     159828.69   \n",
      "\n",
      "   metering_point_count  \n",
      "0                100607  \n",
      "1                100607  \n",
      "2                100607  \n",
      "3                100607  \n",
      "4                100607  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cons = pd.DataFrame(all_consumption_records)\n",
    "\n",
    "# Keep only needed fields and rename to target format\n",
    "df_cons = df_cons.rename(columns={\n",
    "    \"priceArea\": \"price_area\",\n",
    "    \"consumptionGroup\": \"consumption_group\",\n",
    "    \"startTime\": \"start_time\",\n",
    "    \"quantityKwh\": \"quantity_kwh\",\n",
    "    \"meteringPointCount\": \"metering_point_count\"\n",
    "})[[\n",
    "    \"price_area\",\n",
    "    \"consumption_group\",\n",
    "    \"start_time\",\n",
    "    \"quantity_kwh\",\n",
    "    \"metering_point_count\"\n",
    "]]\n",
    "\n",
    "# Convert start_time to datetime\n",
    "df_cons[\"start_time\"] = pd.to_datetime(df_cons[\"start_time\"], utc=True)\n",
    "\n",
    "print(df_cons.dtypes)\n",
    "print(df_cons.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc8a4a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876600 hourly consumption records inserted (2021‚Äì2024).\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = MongoClient(os.getenv(\"MONGO_URI\"), server_api=ServerApi('1'))\n",
    "db = client[\"elhub_data\"]\n",
    "collection = db[\"consumption_data\"]  # NEW TABLE (COLLECTION)\n",
    "\n",
    "# Transform DataFrame to list of dicts and insert\n",
    "data_for_mongo = df_cons.to_dict(\"records\")\n",
    "collection.insert_many(data_for_mongo)\n",
    "\n",
    "print(f\"{len(data_for_mongo)} hourly consumption records inserted (2021‚Äì2024).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dc70f",
   "metadata": {},
   "source": [
    "#### In Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ad4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
